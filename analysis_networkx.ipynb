{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFL and Citibike Networkx Analysis \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the relevant libraries\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis introduction\n",
    "---\n",
    "\n",
    "\n",
    "NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks, often also known as graphs. https://networkx.org/\n",
    "\n",
    "### Centrality measures \n",
    "- Centrality measures are a vital tool for understanding networks.\n",
    "- These algorithms use graph theory to calculate the importance of any given node in a network. They cut through noisy data, revealing parts of the network that need attention – but they all work differently. Each measure has its own definition of ‘importance’, so you need to understand how they work to find the best one for your graph visualization applications.\n",
    "- The follow details from each centrality measure is taken from Cambridge Intelligence, and is based on analysing a human ~social network https://cambridge-intelligence.com/keylines-faqs-social-network-analysis/\n",
    "___ \n",
    "##### 1. Degree Centrality\n",
    "- **Definition:** Degree centrality assigns an importance score based simply on the number of links held by each node.\n",
    "- **What it tells us:** How many direct, ‘one hop’ connections each node has to other nodes in the network.\n",
    "- **When to use it:** For finding very connected individuals, popular individuals, individuals who are likely to hold most information or individuals who can quickly connect with the wider network.\n",
    "- **A bit more detail:** Degree centrality is the simplest measure of node connectivity. Sometimes it’s useful to look at in-degree (number of inbound links) and out-degree (number of outbound links) as distinct measures, for example when looking at transactional data or account activity.\n",
    "\n",
    "##### 2. Closeness centrality\n",
    "- **Definition:** Closeness centrality scores each node based on their ‘closeness’ to all other nodes in the network.\n",
    "- **What it tells us:** This measure calculates the shortest paths between all nodes, then assigns each node a score based on its sum of shortest paths.\n",
    "- **When to use it:** For finding the individuals who are best placed to influence the entire network most quickly.\n",
    "- **A bit more detail:** Closeness centrality can help find good ‘broadcasters’, but in a highly-connected network, you will often find all nodes have a similar score. What may be more useful is using Closeness to find influencers in a single cluster.\n",
    "\n",
    "##### 3. Eigen Centrality\n",
    "- **Definition:** Like degree centrality, EigenCentrality measures a node’s influence based on the number of links it has to other nodes in the network. EigenCentrality then goes a step further by also taking into account how well connected a node is, and how many links their connections have, and so on through the network.\n",
    "- **What it tells us:** By calculating the extended connections of a node, EigenCentrality can identify nodes with influence over the whole network, not just those directly connected to it.\n",
    "- **When to use it:** EigenCentrality is a good ‘all-round’ SNA score, handy for understanding human social networks, but also for understanding networks like malware propagation.\n",
    "- **A bit more detail:** Our tools calculate each node’s EigenCentrality by converging on an eigenvector using the power iteration method. Learn more about EigenCentrality\n",
    "\n",
    "##### 4. PageRank\n",
    "- **Definition:** PageRank is a variant of EigenCentrality, also assigning nodes a score based on their connections, and their connections’ connections. The difference is that PageRank also takes link direction and weight into account – so links can only pass influence in one direction, and pass different amounts of influence.\n",
    "- **What it tells us:** This measure uncovers nodes whose influence extends beyond their direct connections into the wider network.\n",
    "- **When to use it:** Because it takes into account direction and connection weight, PageRank can be helpful for understanding citations and authority.\n",
    "- **A bit more detail:** PageRank is famously one of the ranking algorithms behind the original Google search engine (the ‘Page’ part of its name comes from creator and Google founder, Larry Page).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# TfL bikes\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection to postgres database\n",
    "conn = psycopg2.connect(\n",
    "    user=\"postgres\",\n",
    "    password=\"password123\",\n",
    "    host=\"localhost\",\n",
    "    database=\"diss_data\",\n",
    ")\n",
    "\n",
    "engine = sqlalchemy.create_engine(\"postgresql://postgres:password123@localhost:5432/diss_data\")\n",
    "\n",
    "conn = psycopg2.connect(database=\"diss_data\", user=\"postgres\", password=\"password123\", host=\"localhost\", port=\"5432\")\n",
    "# define the SQL query to retrieve the data from the table\n",
    "sql_query = \"SELECT * FROM bike_data_2019_tb_v03\"\n",
    "# use the read_sql function to read the table into a Pandas dataframe\n",
    "df = pd.read_sql(sql_query, conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2022\n",
    "sql_query3 = \"SELECT * FROM bike_data_2022_tb_v06\"\n",
    "df3 = pd.read_sql(sql_query3, conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying the dataframe\n",
    "bike_data_2019 = df.copy()\n",
    "bike_data_2022 = df3.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following analysis will rely on a weight being assigned to each edge of the network. In this instance TfL bike journey counts will be the weight. For example, the edge connecting docking station A to docking station B, will use the total jounery count between A and B over 1 year\n",
    "Let's create a dataframe that holds all the relevant inforamtion we need  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a network graph object\n",
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding nodes \n",
    "# Calculate the edge weights based on the total number of journeys between each combination of start and end stations\n",
    "edge_weights = bike_data_2022.groupby([\"StartStation Name\", \"EndStation Name\"]).size().reset_index(name=\"weight\")\n",
    "\n",
    "# Add nodes to the graph from StartStation and EndStation columns\n",
    "start_stations = bike_data_2022[\"StartStation Name\"].unique()\n",
    "end_stations = bike_data_2022[\"EndStation Name\"].unique()\n",
    "G.add_nodes_from(start_stations)\n",
    "G.add_nodes_from(end_stations)\n",
    "\n",
    "# Add edges to the graph using the calculated edge weights\n",
    "edges = edge_weights[[\"StartStation Name\", \"EndStation Name\", \"weight\"]].values\n",
    "G.add_weighted_edges_from(edges)\n",
    "\n",
    "# Perform network analysis using NetworkX functions\n",
    "num_nodes = G.number_of_nodes()\n",
    "num_edges = G.number_of_edges()\n",
    "avg_degree = sum(d for _, d in G.degree()) / float(num_nodes)\n",
    "average_weight = sum(nx.get_edge_attributes(G, \"weight\").values()) / G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nx.draw(G, node_size=100, node_color=\"lightblue\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "directed graph - for simplicity - we didn't create a directed graph\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree centrality\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "# Calculate betweenness centrality\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "# Calculate closeness centrality\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "# Calculate eigenvector centrality\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "#page rank\n",
    "pagerank_centrality = nx.pagerank(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort degree centrality dictionary by value\n",
    "sorted_degree_centrality = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "# Sort betweenness centrality dictionary by value\n",
    "sorted_betweenness_centrality = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "# Sort closeness centrality dictionary by value\n",
    "sorted_closeness_centrality = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "# Sort eigenvector centrality dictionary by value\n",
    "sorted_eigenvector_centrality = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "# Sort pagerank centrality dictionary by value\n",
    "sorted_pagerank_centrality = sorted(pagerank_centrality.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_degree_centrality\n",
    "#sorted_betweenness_centrality\n",
    "#sorted_closeness_centrality\n",
    "#sorted_eigenvector_centrality\n",
    "#sorted_pagerank_centrality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting the centrality results to csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = [sorted_degree_centrality, sorted_betweenness_centrality, sorted_closeness_centrality, sorted_eigenvector_centrality, sorted_pagerank_centrality]\n",
    "centrality_names = [\"degree_centrality\", \"betweenness_centrality\", \"closeness_centrality\", \"eigenvector_centrality\", \"pagerank_centrality\"]\n",
    "\n",
    "for name, centrality_list in zip(centrality_names, result_list):\n",
    "    # Convert the inner list to a DataFrame\n",
    "    df = pd.DataFrame(centrality_list)\n",
    "\n",
    "    # Save the DataFrame as a CSV file named after the element in the list\n",
    "    #filename = \"C:\\Users\\EMoses\\OneDrive - Birkbeck, University of London\\Documents\\Birkbeck\\2022-23\\Dissertation\\TfL_Bike_Analysis\\output\"+str(name)+\".csv\"\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new NetworkX graph object\n",
    "G_2019 = nx.Graph()\n",
    "\n",
    "# Calculate the edge weights based on the total number of journeys between each combination of start and end stations\n",
    "edge_weights_2019 = bike_data_2019.groupby([\"StartStation Name\", \"EndStation Name\"]).size().reset_index(name=\"weight\")\n",
    "\n",
    "# Add nodes to the graph from StartStation and EndStation columns\n",
    "start_stations = bike_data_2019[\"StartStation Name\"].unique()\n",
    "end_stations = bike_data_2019[\"EndStation Name\"].unique()\n",
    "G_2019.add_nodes_from(start_stations)\n",
    "G_2019.add_nodes_from(end_stations)\n",
    "\n",
    "# Add edges to the graph using the calculated edge weights\n",
    "edges_2019 = edge_weights_2019[[\"StartStation Name\", \"EndStation Name\", \"weight\"]].values\n",
    "G_2019.add_weighted_edges_from(edges_2019)\n",
    "\n",
    "# Add edges to the graph using Rental Id as the edge identifier\n",
    "edges_2019 = bike_data_2019[[\"StartStation Name\", \"EndStation Name\"]].values.tolist()\n",
    "G_2019.add_edges_from(edges_2019)\n",
    "\n",
    "# Perform network analysis using NetworkX functions\n",
    "num_nodes = G_2019.number_of_nodes()\n",
    "num_edges = G_2019.number_of_edges()\n",
    "avg_degree = sum(d for _, d in G_2019.degree()) / float(num_nodes)\n",
    "average_weight = sum(nx.get_edge_attributes(G_2019, \"weight\").values()) / G_2019.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes\n",
    "#num_edges\n",
    "#avg_degree \n",
    "#average_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree centrality\n",
    "degree_centrality_2019 = nx.degree_centrality(G_2019)\n",
    "# Calculate betweenness centrality\n",
    "betweenness_centrality_2019 = nx.betweenness_centrality(G_2019)\n",
    "# Calculate closeness centrality\n",
    "closeness_centrality_2019 = nx.closeness_centrality(G_2019)\n",
    "# Calculate eigenvector centrality\n",
    "eigenvector_centrality_2019 = nx.eigenvector_centrality(G_2019)\n",
    "#page rank\n",
    "pagerank_centrality_2019 = nx.pagerank(G_2019)\n",
    "\n",
    "\n",
    "# Sort degree centrality dictionary by value\n",
    "sorted_degree_centrality_2019 = sorted(degree_centrality_2019.items(), key=lambda x: x[1], reverse=True)\n",
    "# Sort betweenness centrality dictionary by value\n",
    "sorted_betweenness_centrality_2019  = sorted(betweenness_centrality_2019.items(), key=lambda x: x[1], reverse=True)\n",
    "# Sort closeness centrality dictionary by value\n",
    "sorted_closeness_centrality_2019  = sorted(closeness_centrality_2019.items(), key=lambda x: x[1], reverse=True)\n",
    "# Sort eigenvector centrality dictionary by value\n",
    "sorted_eigenvector_centrality_2019  = sorted(eigenvector_centrality_2019.items(), key=lambda x: x[1], reverse=True)\n",
    "# Sort pagerank centrality dictionary by value\n",
    "sorted_pagerank_centrality_2019 = sorted(pagerank_centrality_2019.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_degree_centrality_2019\n",
    "#sorted_betweenness_centrality_2019\n",
    "#sorted_closeness_centrality_2019  \n",
    "#sorted_eigenvector_centrality_2019\n",
    "#sorted_pagerank_centrality_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping the results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the bike station locations\n",
    "##### TfL have a live \"cycle hire updates\" feed which lists information for each cycle hire station, updated once every minute or so. I don't utilise this live data - instead I just take the name, ID, lat/lon, and capacity for each bike station.\n",
    "\n",
    "Code adopted from https://github.com/charlie1347/TfL_bikes/blob/master/TfL%20Bikes.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from xml.etree import ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "site = \"https://tfl.gov.uk/tfl/syndication/feeds/cycle-hire/livecyclehireupdates.xml\"\n",
    "\n",
    "response = requests.get(site)\n",
    "root = ET.fromstring(response.content)\n",
    "\n",
    "id_list = [int(root[i][0].text) for i in range(0, len(root))]\n",
    "name_list = [root[i][1].text for i in range(0, len(root))]\n",
    "lat_list = [float(root[i][3].text) for i in range(0, len(root))]\n",
    "lon_list = [float(root[i][4].text) for i in range(0, len(root))]\n",
    "#capacity_list = [int(root[i][12].text) for i in range(0, len(root))]\n",
    "\n",
    "#\n",
    "all_locs = pd.DataFrame(list(zip(name_list, id_list, lat_list, \n",
    "                                 lon_list)), columns = [\"name\",\"id\",\"lat\",\"lon\"])\n",
    "\n",
    "#conver to csv\n",
    "#all_locs.to_csv(\"output/bike_point_locations_saved.csv\", header=True, index=None)\n",
    "\n",
    "print(all_locs.shape)\n",
    "\n",
    "\n",
    "locations = all_locs.copy()\n",
    "\n",
    "locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an static network map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = list(zip(locations[\"lon\"],locations[\"lat\"]))\n",
    "pos = dict(zip(locations[\"name\"], coords))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterate over the items in the pos dictionary, and for each node name and position, we check if the position is not None. If the position is not None, we add the node to the graph with its corresponding position. Otherwise, we print a message indicating that the node is being skipped due to the missing position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing nodes with no position \n",
    "#we will add the locations nodes withouts positons later \n",
    "G_clean = nx.Graph()\n",
    "\n",
    "for node_name, position in pos.items():\n",
    "    if position is not None:\n",
    "        G_clean.add_node(node_name, pos=position)\n",
    "    else:\n",
    "        print(f\"Skipping node \"{node_name}\" due to missing position.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G_clean,pos,node_size=10, alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the original graph \"original_graph\" and the graph with node positions \"G\"\n",
    "\n",
    "# Copy the original graph to the new graph with node positions\n",
    "G_with_edges = G_clean.copy()\n",
    "\n",
    "# Iterate over the edges of the original graph\n",
    "for edge in G.edges:\n",
    "    node1, node2 = edge\n",
    "\n",
    "    # Check if both nodes exist in the new graph with positions\n",
    "    if node1 in G_with_edges and node2 in G_with_edges:\n",
    "        # Add the edge to the new graph with positions\n",
    "        G_with_edges.add_edge(node1, node2)\n",
    "\n",
    "# Plot the graph with edges and node positions\n",
    "pos = nx.get_node_attributes(G_with_edges, \"pos\")\n",
    "fig, ax = plt.subplots(figsize=(24, 20))\n",
    "nx.draw(G_with_edges, pos, node_size=10, alpha=.5, width=0.0015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an interactive map using folium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for each centrality measure\n",
    "degree_df = pd.DataFrame.from_dict(degree_centrality, orient=\"index\", columns=[\"degree_centrality\"])\n",
    "betweenness_df = pd.DataFrame.from_dict(betweenness_centrality, orient=\"index\", columns=[\"betweenness_centrality\"])\n",
    "closeness_df = pd.DataFrame.from_dict(closeness_centrality, orient=\"index\", columns=[\"closeness_centrality\"])\n",
    "eigenvector_df = pd.DataFrame.from_dict(eigenvector_centrality, orient=\"index\", columns=[\"eigenvector_centrality\"])\n",
    "pagerank_centrality_df = pd.DataFrame.from_dict(pagerank_centrality, orient=\"index\", columns=[\"pagerank_centrality\"])\n",
    "\n",
    "locations_cen = locations.copy()\n",
    "# Merge centrality measures with the locations DataFrame based on the shared docking station name\n",
    "locations_cen = pd.merge(locations_cen, degree_df, left_on=\"name\", right_index=True, how=\"left\")\n",
    "locations_cen = pd.merge(locations_cen, betweenness_df, left_on=\"name\", right_index=True, how=\"left\")\n",
    "locations_cen = pd.merge(locations_cen, closeness_df, left_on=\"name\", right_index=True, how=\"left\")\n",
    "locations_cen = pd.merge(locations_cen, eigenvector_df, left_on=\"name\", right_index=True, how=\"left\")\n",
    "locations_cen = pd.merge(locations_cen, pagerank_centrality_df, left_on=\"name\", right_index=True, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count and then Drop rows with any NaN values\n",
    "na_counts = locations_cen.isna().sum()\n",
    "locations_cen = locations_cen.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_cen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to csv\n",
    "locations_cen.to_csv(\"output/centrality_and_locations.csv\", header=True, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the color ramp from light yellow to dark red\n",
    "color_ramp = cm.get_cmap(\"YlOrRd\")\n",
    "\n",
    "# Creating a folium map\n",
    "m = folium.Map(\n",
    "    location=[51.5074, -0.1272],  # Start location as lat and lon\n",
    "    tiles=\"CartoDB dark_matter\",  # Adding a dark basemap\n",
    "    zoom_start=12,  # Level of zoom\n",
    "    prefer_canvas=True,  # Useful for changing the base map\n",
    ")\n",
    "\n",
    "# Calculate the maximum degree centrality value for scaling\n",
    "max_degree_centrality = locations_cen[\"pagerank_centrality\"].max()\n",
    "\n",
    "# Iterate through every row of the DataFrame using the iterrows() function\n",
    "for index, val in locations_cen.iterrows():\n",
    "    # Create a popup string with the bike point name and network analysis results\n",
    "    popup_string = f\"Name: {val[\"name\"]}<br>\"\n",
    "    popup_string += f\"Degree Centrality: {val[\"degree_centrality\"]:.4f}<br>\"\n",
    "    popup_string += f\"Betweenness Centrality: {val[\"betweenness_centrality\"]:.4f}<br>\"\n",
    "    popup_string += f\"Eigenvector Centrality: {val[\"eigenvector_centrality\"]:.4f}\"\n",
    "\n",
    "    # Calculate the scaled size based on the degree centrality value\n",
    "    size = 5 * (val[\"pagerank_centrality\"] / max_degree_centrality)\n",
    "\n",
    "    # Calculate the color based on the degree centrality value using the color ramp\n",
    "    normalized_value = val[\"pagerank_centrality\"] / max_degree_centrality\n",
    "    color = colors.rgb2hex(color_ramp(normalized_value)[:3])\n",
    "\n",
    "    folium.CircleMarker(\n",
    "        location=[val[\"lat\"], val[\"lon\"]],\n",
    "        # Styling the circles with different parameters\n",
    "        radius=size,\n",
    "        popup=popup_string,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_opacity=0.7\n",
    "    # Adding all circles to the map\n",
    "    ).add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Citi bikes\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the events data from pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_path = Path(\"data/citibike_pickle/bike_data_2019.p\")\n",
    "bike_data_2019 = pd.read_pickle(events_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_path = Path(\"data/citibike_pickle/bike_data_2022.p\")\n",
    "bike_data_2022 = pd.read_pickle(events_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new NetworkX graph object\n",
    "G_2019 = nx.Graph()\n",
    "\n",
    "# Calculate the edge weights based on the total number of journeys between each combination of start and end stations\n",
    "edge_weights_2019 = bike_data_2019.groupby([\"start_station_name\", \"end_station_name\"]).size().reset_index(name=\"weight\")\n",
    "\n",
    "# Add nodes to the graph from StartStation and EndStation columns\n",
    "start_stations = bike_data_2019[\"start_station_name\"].unique()\n",
    "end_stations = bike_data_2019[\"end_station_name\"].unique()\n",
    "G_2019.add_nodes_from(start_stations)\n",
    "G_2019.add_nodes_from(end_stations)\n",
    "\n",
    "# Add edges to the graph using the calculated edge weights\n",
    "edges_2019 = edge_weights_2019[[\"start_station_name\", \"end_station_name\", \"weight\"]].values\n",
    "G_2019.add_weighted_edges_from(edges_2019)\n",
    "\n",
    "# Add edges to the graph using Rental Id as the edge identifier\n",
    "edges_2019 = bike_data_2019[[\"start_station_name\", \"end_station_name\"]].values.tolist()\n",
    "G_2019.add_edges_from(edges_2019)\n",
    "\n",
    "# Perform network analysis using NetworkX functions\n",
    "num_nodes = G_2019.number_of_nodes()\n",
    "num_edges = G_2019.number_of_edges()\n",
    "avg_degree = sum(d for _, d in G_2019.degree()) / float(num_nodes)\n",
    "average_weight = sum(nx.get_edge_attributes(G_2019, \"weight\").values()) / G_2019.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes\n",
    "#num_edges\n",
    "#avg_degree \n",
    "#average_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree centrality\n",
    "degree_centrality_2019 = nx.degree_centrality(G_2019)\n",
    "# Calculate betweenness centrality\n",
    "betweenness_centrality_2019 = nx.betweenness_centrality(G_2019)\n",
    "# Calculate closeness centrality\n",
    "closeness_centrality_2019 = nx.closeness_centrality(G_2019)\n",
    "# Calculate eigenvector centrality\n",
    "eigenvector_centrality_2019 = nx.eigenvector_centrality(G_2019)\n",
    "#page rank\n",
    "pagerank_centrality_2019 = nx.pagerank(G_2019)\n",
    "\n",
    "\n",
    "# Sort degree centrality dictionary by value\n",
    "sorted_degree_centrality_2019 = sorted(degree_centrality_2019.items(), key=lambda x: x[1], reverse=True)\n",
    "# Sort betweenness centrality dictionary by value\n",
    "sorted_betweenness_centrality_2019  = sorted(betweenness_centrality_2019.items(), key=lambda x: x[1], reverse=True)\n",
    "# Sort closeness centrality dictionary by value\n",
    "sorted_closeness_centrality_2019  = sorted(closeness_centrality_2019.items(), key=lambda x: x[1], reverse=True)\n",
    "# Sort eigenvector centrality dictionary by value\n",
    "sorted_eigenvector_centrality_2019  = sorted(eigenvector_centrality_2019.items(), key=lambda x: x[1], reverse=True)\n",
    "# Sort pagerank centrality dictionary by value\n",
    "sorted_pagerank_centrality_2019 = sorted(pagerank_centrality_2019.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted_degree_centrality_2019\n",
    "#sorted_betweenness_centrality_2019\n",
    "#sorted_closeness_centrality_2019  \n",
    "#sorted_eigenvector_centrality_2019\n",
    "sorted_pagerank_centrality_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise network\n",
    "nx.draw(G_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting 2019 centrality measures as csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = [sorted_degree_centrality_2019, sorted_betweenness_centrality_2019, sorted_closeness_centrality_2019, sorted_eigenvector_centrality_2019, sorted_pagerank_centrality_2019]\n",
    "centrality_names = [\"degree_centrality_2019\", \"betweenness_centrality_2019\", \"closeness_centrality_2019\", \"eigenvector_centrality_2019\", \"pagerank_centrality_2019\"]\n",
    "\n",
    "for name, centrality_list in zip(centrality_names, result_list):\n",
    "    # Convert the inner list to a DataFrame\n",
    "    df = pd.DataFrame(centrality_list)\n",
    "\n",
    "    # Save the DataFrame as a CSV file named after the element in the list\n",
    "    filename = \"data\\\\citibike_network_output\\\\\"+str(name)+\".csv\"\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new NetworkX graph object\n",
    "G_2022 = nx.Graph()\n",
    "\n",
    "# Calculate the edge weights based on the total number of journeys between each combination of start and end stations\n",
    "edge_weights_2022 = bike_data_2022.groupby([\"start_station_name\", \"end_station_name\"]).size().reset_index(name=\"weight\")\n",
    "\n",
    "# Add nodes to the graph from StartStation and EndStation columns\n",
    "start_stations = bike_data_2022[\"start_station_name\"].unique()\n",
    "end_stations = bike_data_2022[\"end_station_name\"].unique()\n",
    "G_2022.add_nodes_from(start_stations)\n",
    "G_2022.add_nodes_from(end_stations)\n",
    "\n",
    "# Add edges to the graph using the calculated edge weights\n",
    "edges_2022 = edge_weights_2022[[\"start_station_name\", \"end_station_name\", \"weight\"]].values\n",
    "G_2022.add_weighted_edges_from(edges_2022)\n",
    "\n",
    "# Add edges to the graph using Rental Id as the edge identifier\n",
    "edges_2022 = bike_data_2022[[\"start_station_name\", \"end_station_name\"]].values.tolist()\n",
    "G_2022.add_edges_from(edges_2022)\n",
    "\n",
    "# Perform network analysis using NetworkX functions\n",
    "num_nodes = G_2022.number_of_nodes()\n",
    "num_edges = G_2022.number_of_edges()\n",
    "avg_degree = sum(d for _, d in G_2022.degree()) / float(num_nodes)\n",
    "average_weight = sum(nx.get_edge_attributes(G_2022, \"weight\").values()) / G_2022.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new NetworkX graph object\n",
    "G_2022 = nx.Graph()\n",
    "\n",
    "# Calculate the edge weights based on the total number of journeys between each combination of start and end stations\n",
    "edge_weights_2022 = bike_data_2022.groupby([\"start_station_name\", \"end_station_name\"]).size().reset_index(name=\"weight\")\n",
    "\n",
    "# Extract unique start and end stations\n",
    "start_stations = bike_data_2022[\"start_station_name\"].unique()\n",
    "end_stations = bike_data_2022[\"end_station_name\"].unique()\n",
    "\n",
    "# Add nodes to the graph from start and end stations\n",
    "G_2022.add_nodes_from(start_stations)\n",
    "G_2022.add_nodes_from(end_stations)\n",
    "\n",
    "# Prepare edges with weights\n",
    "edges_2022 = edge_weights_2022[[\"start_station_name\", \"end_station_name\", \"weight\"]].values\n",
    "\n",
    "# Add weighted edges to the graph\n",
    "G_2022.add_weighted_edges_from(edges_2022)\n",
    "\n",
    "# Calculate number of nodes\n",
    "num_nodes = G_2022.number_of_nodes()\n",
    "\n",
    "# Calculate number of edges\n",
    "num_edges = G_2022.number_of_edges()\n",
    "\n",
    "# Calculate average degree\n",
    "avg_degree = sum(dict(G_2022.degree()).values()) / float(num_nodes)\n",
    "\n",
    "# Calculate average weight of edges\n",
    "average_weight = sum(nx.get_edge_attributes(G_2022, \"weight\").values()) / G_2022.number_of_edges()\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of nodes: {num_nodes}\")\n",
    "print(f\"Number of edges: {num_edges}\")\n",
    "print(f\"Average degree: {avg_degree:.2f}\")\n",
    "print(f\"Average edge weight: {average_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_nodes\n",
    "#num_edges\n",
    "#avg_degree \n",
    "#average_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree centrality\n",
    "degree_centrality_2022 = nx.degree_centrality(G_2022)\n",
    "# Calculate betweenness centrality\n",
    "betweenness_centrality_2022 = nx.betweenness_centrality(G_2022)\n",
    "# Calculate closeness centrality\n",
    "closeness_centrality_2022 = nx.closeness_centrality(G_2022)\n",
    "# Calculate eigenvector centrality\n",
    "eigenvector_centrality_2022 = nx.eigenvector_centrality(G_2022)\n",
    "#page rank\n",
    "pagerank_centrality_2022 = nx.pagerank(G_2022)\n",
    "\n",
    "\n",
    "# Sort degree centrality dictionary by value\n",
    "sorted_degree_centrality_2022 = sorted(degree_centrality_2022.items(), key=lambda x: x[1], reverse=True)\n",
    "# Sort betweenness centrality dictionary by value\n",
    "sorted_betweenness_centrality_2022  = sorted(betweenness_centrality_2022.items(), key=lambda x: x[1], reverse=True)\n",
    "# Sort closeness centrality dictionary by value\n",
    "sorted_closeness_centrality_2022  = sorted(closeness_centrality_2022.items(), key=lambda x: x[1], reverse=True)\n",
    "# Sort eigenvector centrality dictionary by value\n",
    "sorted_eigenvector_centrality_2022  = sorted(eigenvector_centrality_2022.items(), key=lambda x: x[1], reverse=True)\n",
    "# Sort pagerank centrality dictionary by value\n",
    "sorted_pagerank_centrality_2022 = sorted(pagerank_centrality_2022.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_degree_centrality_2022\n",
    "#sorted_betweenness_centrality_2022\n",
    "#sorted_closeness_centrality_2022  \n",
    "#sorted_eigenvector_centrality_2022\n",
    "#sorted_pagerank_centrality_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise network\n",
    "nx.draw(G_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new NetworkX graph object\n",
    "G_2022 = nx.Graph()\n",
    "\n",
    "# Calculate the edge weights based on the total number of journeys between each combination of start and end stations\n",
    "edge_weights_2022 = bike_data_2022.groupby([\"start_station_name\", \"end_station_name\"]).size().reset_index(name=\"weight\")\n",
    "\n",
    "# Extract unique start and end stations\n",
    "start_stations = bike_data_2022[\"start_station_name\"].unique()\n",
    "end_stations = bike_data_2022[\"end_station_name\"].unique()\n",
    "\n",
    "# Add nodes to the graph from start and end stations\n",
    "G_2022.add_nodes_from(start_stations)\n",
    "G_2022.add_nodes_from(end_stations)\n",
    "\n",
    "# Prepare edges with weights\n",
    "edges_2022 = edge_weights_2022[[\"start_station_name\", \"end_station_name\", \"weight\"]].values\n",
    "\n",
    "# Add weighted edges to the graph\n",
    "G_2022.add_weighted_edges_from(edges_2022)\n",
    "\n",
    "# Verify the graph structure\n",
    "print(\"Nodes in the graph:\", G_2022.nodes())\n",
    "print(\"Edges in the graph:\", G_2022.edges(data=True))\n",
    "\n",
    "# Calculate centrality measures\n",
    "degree_centrality_2022 = nx.degree_centrality(G_2022)\n",
    "betweenness_centrality_2022 = nx.betweenness_centrality(G_2022)\n",
    "closeness_centrality_2022 = nx.closeness_centrality(G_2022)\n",
    "eigenvector_centrality_2022 = nx.eigenvector_centrality(G_2022)\n",
    "pagerank_centrality_2022 = nx.pagerank(G_2022)\n",
    "\n",
    "# Sort centrality measures\n",
    "sorted_degree_centrality_2022 = sorted(degree_centrality_2022.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_betweenness_centrality_2022 = sorted(betweenness_centrality_2022.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_closeness_centrality_2022 = sorted(closeness_centrality_2022.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_eigenvector_centrality_2022 = sorted(eigenvector_centrality_2022.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_pagerank_centrality_2022 = sorted(pagerank_centrality_2022.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print sorted centrality measures\n",
    "print(\"\\nSorted Degree Centrality:\", sorted_degree_centrality_2022)\n",
    "print(\"\\nSorted Betweenness Centrality:\", sorted_betweenness_centrality_2022)\n",
    "print(\"\\nSorted Closeness Centrality:\", sorted_closeness_centrality_2022)\n",
    "print(\"\\nSorted Eigenvector Centrality:\", sorted_eigenvector_centrality_2022)\n",
    "print(\"\\nSorted PageRank Centrality:\", sorted_pagerank_centrality_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting 2022 centrality measures as csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = [sorted_degree_centrality_2022, sorted_betweenness_centrality_2022, sorted_closeness_centrality_2022, sorted_eigenvector_centrality_2022, sorted_pagerank_centrality_2022]\n",
    "centrality_names = [\"degree_centrality_2022\", \"betweenness_centrality_2022\", \"closeness_centrality_2022\", \"eigenvector_centrality_2022\", \"pagerank_centrality_2022\"]\n",
    "\n",
    "for name, centrality_list in zip(centrality_names, result_list):\n",
    "    # Convert the inner list to a DataFrame\n",
    "    df = pd.DataFrame(centrality_list)\n",
    "\n",
    "    # Save the DataFrame as a CSV file named after the element in the list\n",
    "    filename = \"data\\\\citibike_network_output\\\\\"+str(name)+\".csv\"\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Superseded\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an interactive plot using Flow-map in KeplerGL\n",
    "- as per: https://towardsdatascience.com/visualization-of-bike-sharing-system-movements-in-helsinki-with-an-interactive-flow-map-451d897104fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from keplergl import KeplerGl\n",
    "from pyproj import CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unwanted columns \n",
    "bike_data_2022_clean = bike_data_2022.copy()\n",
    "\n",
    "unwanted_columns = [\"Rental Id\", \"Duration\", \"Bike Id\", \"EndStation Id\", \"StartStation Id\", \"SS Terminal Name\", \"ES Terminal Name\", \"Bike model\", \"Hour\" , \"Day\"]\n",
    "\n",
    "# Step 2: Use the drop() method to remove the unwanted columns\n",
    "bike_data_2022_clean = bike_data_2022_clean.drop(columns=unwanted_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data_2022_clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a geodataframe using the location dataframe \n",
    "\n",
    "# Create a geometry column based on lat and lon\n",
    "geometry = [Point(lon, lat) for lon, lat in zip(locations[\"lon\"], locations[\"lat\"])]\n",
    "\n",
    "# Create the GeoDataFrame\n",
    "locations_gdf = gpd.GeoDataFrame(locations, geometry=geometry)\n",
    "\n",
    "# Set the coordinate reference system (CRS) if needed. For example, for WGS84 (EPSG: 4326)\n",
    "locations_gdf.crs = \"EPSG:4326\"\n",
    "\n",
    "# Now, the GeoDataFrame \"gdf\" contains all the original columns, and a \"geometry\" column based on lat and lon.\n",
    "locations_gdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_gdf.plot(markersize=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merging the trip and location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the locations_gdf DataFrame by converting \"ID\" to int and adding suffixes\n",
    "locations_gdf[\"id\"] = locations_gdf[\"id\"].astype(int)\n",
    "geodata_origin = locations_gdf.add_suffix(\"_origin\")\n",
    "geodata_destin = locations_gdf.add_suffix(\"_destin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creatiung a dictionary of station names and ids \n",
    "id_dict = {}\n",
    "\n",
    "# Step 2: Iterate through the DataFrame and populate the dictionary\n",
    "for index, row in geodata_origin.iterrows():\n",
    "    name = row[\"name_origin\"]\n",
    "    id_origin = row[\"id_origin\"]\n",
    "    id_dict[name] = id_origin\n",
    "\n",
    "# Display the resulting dictionary\n",
    "print(id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data_2022_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the \"StartStation_id\" column by mapping the \"StartStation Name\" column to the dictionary values\n",
    "bike_data_2022_clean[\"StartStation_id\"] = bike_data_2022_clean[\"StartStation Name\"].map(id_dict)\n",
    "bike_data_2022_clean[\"EndStation_id\"] = bike_data_2022_clean[\"EndStation Name\"].map(id_dict)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "bike_data_2022_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Merge data with no aggregation based on \"Departure station id\"\n",
    "data_merge = bike_data_2022_clean.merge(geodata_origin, left_on=\"StartStation_id\", right_on=\"id_origin\", how=\"outer\")\n",
    "\n",
    "# Step 3: Merge data with no aggregation based on \"Return station id\"\n",
    "data_merge = data_merge.merge(geodata_destin, left_on=\"EndStation_id\", right_on=\"id_destin\", how=\"outer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Select the needed columns for the final coor_data DataFrame\n",
    "data_merge= data_merge[[\"Start Date\", \"End Date\", \"StartStation_id\", \"StartStation Name\", \"EndStation_id\",\n",
    "                       \"EndStation Name\", \"lon_origin\", \"lat_origin\", \"lon_destin\", \"lat_destin\", \"geometry_origin\"]]\n",
    "\n",
    "# Display the updated DataFrame\n",
    "data_merge.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aggregate the trips between Bike Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data = data_merge.copy()\n",
    "\n",
    "# aggregating data\n",
    "# creating pairs\n",
    "# if 457 stations, then we make a lot of pairs.\n",
    "origins = locations_gdf[\"id\"].to_list()\n",
    "destins = locations_gdf[\"id\"].to_list()\n",
    "\n",
    "# creating pairs of trips\n",
    "pairs_list = []\n",
    "for origin in origins:\n",
    "    for destin in destins:\n",
    "        stat_pair = (int(origin), int(destin))\n",
    "        pairs_list.append(stat_pair)\n",
    "\n",
    "# dictionary with pairs and ID\n",
    "routes_dict = dict(zip(pairs_list, range(len(pairs_list))))\n",
    "\n",
    "agg_data = agg_data.dropna()\n",
    "# adding a column with station pairs\n",
    "agg_data[\"pairs\"] = [(int(orID), int(deID)) for orID, deID in zip(agg_data[\"StartStation_id\"].to_list(),\n",
    "                                                                  agg_data[\"EndStation_id\"].to_list())]\n",
    "\n",
    "agg_data = agg_data.reset_index(drop=True)\n",
    "# add route id through dictionary\n",
    "agg_data[\"route_id\"] = None\n",
    "for i in range(len(agg_data)):\n",
    "    pair_route = agg_data.at[i, \"pairs\"]\n",
    "    route_id = routes_dict[pair_route]\n",
    "    agg_data.at[i, \"route_id\"] = route_id\n",
    "\n",
    "agg_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next step is to aggregate by counting the ID as movers. It goes like this in a grouped DataFrame\n",
    "# aggregating movers\n",
    "movers = gpd.GeoDataFrame(index = agg_data[\"route_id\"].unique())\n",
    "for key, group in agg_data.groupby(\"route_id\"):\n",
    " \n",
    " movers.at[key, \"route_id\"] = key\n",
    " \n",
    " movers.at[key, \"movers\"] = int(len(group))\n",
    " \n",
    " movers.at[key, \"origin_id\"] = group[\"StartStation_id\"].unique()\n",
    " movers.at[key, \"origin_name\"] = group[\"StartStation Name\"].unique()[0]\n",
    " movers.at[key, \"destin_id\"] = group[\"EndStation_id\"].unique()\n",
    " movers.at[key, \"destin_name\"] = group[\"EndStation Name\"].unique()[0]\n",
    " \n",
    " movers.at[key, \"lon_origin\"] = group[\"lon_origin\"].unique()[0]\n",
    " movers.at[key, \"lat_origin\"] = group[\"lat_origin\"].unique()[0]\n",
    " movers.at[key, \"lon_destin\"] = group[\"lon_destin\"].unique()[0]\n",
    " movers.at[key, \"lat_destin\"] = group[\"lat_destin\"].unique()[0]\n",
    "movers[\"geometry\"] = [Point(originx, originy) for originx, originy in zip(movers[\"lon_origin\"].to_list(),\n",
    "                                                                          movers[\"lat_origin\"].to_list())]\n",
    " \n",
    "movers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining CRS\n",
    "movers.crs = CRS.from_epsg(4326)\n",
    "# columns\n",
    "movers_clean = movers[['route_id', 'movers', 'lon_origin', 'lat_origin', 'lon_destin', 'lat_destin', 'geometry' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movers_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as csv\n",
    "movers_clean.to_csv('output\\movers_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named \"df\"\n",
    "movers_clean_1000 = movers_clean[movers_clean['movers'] > 1000]\n",
    "movers_clean_1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KeplerGl instance\n",
    "m = KeplerGl(height=800)\n",
    "# Add stop duration\n",
    "m.add_data(filtered_df, 'agg movements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving keplet plots as html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = KeplerGl(\n",
    "    height=600,\n",
    "    data={'agg movements' : movers_clean})\n",
    "\n",
    "\n",
    "m.save_to_html(file_name=\"output\\kepler_plot_2022.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = KeplerGl(\n",
    "    height=600,\n",
    "    data={'agg movements' : movers_clean_1000})\n",
    "\n",
    "\n",
    "m.save_to_html(file_name=\"output\\kepler_plot_2022_1000+.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data_2022_locations = bike_data_2022.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge to obtain start station lat and lon \n",
    "bike_data_2022_locations = bike_data_2022_locations.merge(locations, left_on=\"StartStation Name\", right_on=\"name\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing columns that are no longer needed\n",
    "bike_data_2022_locations_drop1 = bike_data_2022_locations.drop([\"Rental Id\", \"Duration\", \"Bike Id\", \"Bike model\", \"Hour\",\n",
    "                                             \"Day\", \"name\", \"id\"], axis=1)\n",
    "\n",
    "#renaming lat and lon columns \n",
    "bike_data_2022_locations_drop1 = bike_data_2022_locations_drop1.rename(columns={\"lat\": \"origin_lat\", \"lon\": \"origin_lon\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge to obtain end station lat and lon \n",
    "bike_data_2022_locations_drop1 = bike_data_2022_locations_drop1.merge(locations, left_on=\"EndStation Name\", right_on=\"name\", how=\"outer\")\n",
    "# removing columns that are no longer needed\n",
    "bike_data_2022_locations_drop2 = bike_data_2022_locations_drop1.drop([\"name\", \"id\"], axis=1)\n",
    "\n",
    "#renaming lat and lon columns \n",
    "bike_data_2022_locations_drop2 = bike_data_2022_locations_drop2.rename(columns={\"lat\": \"dest_lat\", \"lon\": \"dest_lon\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data_2022_locations_drop2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data_2022_locations_geo = gpd.GeoDataFrame(bike_data_2022_locations_drop2, \n",
    "                                                geometry=gpd.points_from_xy(bike_data_2022_locations_drop2[\"origin_lon\"], \n",
    "                                                                            bike_data_2022_locations_drop2 [\"origin_lat\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data_2022_locations_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values_df = bike_data_2022_locations_geo.isnull()\n",
    "\n",
    "# Count the number of null values in each column\n",
    "null_values_by_column = null_values_df.sum()\n",
    "\n",
    "null_values_by_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_2022 = bike_data_2022_locations_geo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new column by combining values from the start and end docking statons names \n",
    "geo_2022[\"pairs\"] = geo_2022[\"StartStation Name\"] + \", \" + geo_2022[\"EndStation Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_2022"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Droping rows where any of the lat, lon columns have null values \n",
    "This process removes aproximately 400,000 journeys. We will amend this removal later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null values in the specified columns\n",
    "geo_2022_drop = geo_2022.dropna(subset=[\"origin_lat\", \"origin_lon\", \"dest_lat\", \"dest_lon\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculating counts of docking station pairs and adding it to the datframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = geo_2022_drop[\"pairs\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_2022_drop[\"Count\"] = geo_2022_drop[\"pairs\"].map(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movers = geo_2022_drop.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movers = movers.drop_duplicates(subset=[\"pairs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a test datafame\n",
    "movers_test = movers.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining CRS\n",
    "movers.crs = CRS.from_epsg(4326)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.kepler.gl/docs/keplergl-jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KeplerGl instance\n",
    "m = KeplerGl(height=600)\n",
    "# Add stop duration\n",
    "m.add_data(movers_test, \"agg movements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
